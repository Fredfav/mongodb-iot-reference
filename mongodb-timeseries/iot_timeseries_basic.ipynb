{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoT Microdemos\n",
    "\n",
    "\n",
    "## Timeseries\n",
    "A common pattern to store and retrieve time series data is to leverage the document model with the so called bucketing schema pattern. Instead of storing each measurement into a single document, multiple measurements are stored into one single document. This provides the benefits of: \n",
    "* Reducing Storage space (as less data is stored multiple times, e.g. device id and other metadata, as well as better compression ratios on larger documents)\n",
    "* Reduce Index sizes (by bucket size), larger parts of the index will fit into memory and increase performance\n",
    "* Reduce IO by less documents (reading time series at scale is usually IO-bound load)\n",
    "\n",
    "The following examples will guide through the typical patterns:\n",
    "* [Ingesting Data](#Ingesting-Data)\n",
    "* [Indexing Data](#Indexing-Strategy)\n",
    "* [Querying Data](Querying-the-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Data\n",
    "\n",
    "The following statement will search for a document of device 4711 and where the count of measurements is less than 3 entries in the bucket. In reality, this will be a higher number, e.g. 60 or 100. The new measurement is pushed to the array called m. \n",
    "\n",
    "Because of the upsert option, a new document will be inserted, if no available bucket can be found. Increasing the cnt by one during each insert will automatically create a new document once the exiting bucket is full.\n",
    "\n",
    "### Initialize the database and drop the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import os\n",
    "import datetime\n",
    "import bson\n",
    "from bson.json_util import loads, dumps, RELAXED_JSON_OPTIONS\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "CONNECTIONSTRING = \"localhost:27017\"\n",
    "\n",
    "# Establish Database Connection\n",
    "client = pymongo.MongoClient(CONNECTIONSTRING)\n",
    "db = client.iot\n",
    "collection = db.iot_raw\n",
    "\n",
    "# Drop the collection before we start with the demo\n",
    "collection.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Insert the first measurements:\n",
    "MongoDB Query Language offer rich operators that we leverage here to automatically bucket the data, i.e. we do not store each individual measurement into one document, but store multiple measurements into an array.\n",
    "\n",
    "By using upsert, we automatically start a new bucket, i.e. create a new document if no bucket with additional space can be found. Otherwise, we push the new measurement into the bucket.\n",
    "\n",
    "The following statement will find an open bucket of device 4711, i.e. where the count of measurements is less than 3 entries in the bucket. In reality, this will be a higher number, e.g. 60 or 100. The new measurement is pushed to the array called m, the bucket size is increased by one. For the later query on time ranges, we also store the minimal and maximal timestamp within this bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The timestamp of the new measurement\n",
    "# Note: For better readability, we work with datetime objects. \n",
    "# For higher precision of timestamps, e.g. nanoseconds, \n",
    "# it is recommended to work with decimal values representing seconds and nanoseconds\n",
    "date = datetime.datetime.now()\n",
    "\n",
    "# Add the new measurement to the bucket\n",
    "collection.update_one({\n",
    "  \"device\": 4711,\n",
    "  \"cnt\": { \"$lt\": 3 }\n",
    "},\n",
    "{\n",
    "  \"$push\": { \n",
    "    \"m\": {\n",
    "      \"ts\": date,\n",
    "      \"temperature\": random.randint(0,100),\n",
    "      \"rpm\": random.randint(0,10000),\n",
    "      \"status\": \"operating\"\n",
    "    }\n",
    "  },\n",
    "  \"$max\": { \"max_ts\": date },\n",
    "  \"$min\": { \"min_ts\": date },\n",
    "  \"$inc\": { \"cnt\": 1 }\n",
    "},\n",
    "upsert=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The target document looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result = collection.find_one()\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Add additional measurements\n",
    "\n",
    "Insert some more data in order to have multiple buckets (again, here we use a bucket size of 3, in reality this number will be much higher). We Iinsert four more measurements, so there will be two documents with 3 and 2 measurements, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    date = datetime.datetime.now()\n",
    "    \n",
    "    collection.update_one(\n",
    "        {\n",
    "            \"device\": 4711,\n",
    "            \"cnt\": { \"$lt\": 3 }\n",
    "          },\n",
    "          {\n",
    "            \"$push\": { \n",
    "              \"m\": {\n",
    "                \"ts\": date,\n",
    "                \"temperature\": random.randint(0,100),\n",
    "                \"rpm\": random.randint(0,10000),\n",
    "                \"status\": \"operating\",\n",
    "                  \"new_field\": { \"subfield1\": \"s1\", \"subfield2\": random.randint(0,100)}\n",
    "              }\n",
    "            },\n",
    "            \"$max\": { \"max_ts\": date },\n",
    "            \"$min\": { \"min_ts\": date },\n",
    "            \"$inc\": { \"cnt\": 1 }\n",
    "          },\n",
    "          upsert=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The result will look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = collection.find()\n",
    "\n",
    "for doc in res:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Strategy\n",
    "\n",
    "A proper indexing strategy is key for efficient querying of data. The first index is mandatory for efficient time series queries in historical data. The second one is needed for efficient retreival of the current, i.e. open, bucket for each device. If all device types have the same bucket size, it can be created as a partial index - this will only keep the open buckets in the index. For varying bucket sizes, e.g. per device type, the type could be added to the index. The savings can be huge for large implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient queries per device and timespan\n",
    "result = collection.create_index([(\"device\",pymongo.ASCENDING),\n",
    "                         (\"min_ts\",pymongo.ASCENDING),\n",
    "                         (\"max_ts\",pymongo.ASCENDING)])\n",
    "print(\"Created Index: \" + result)\n",
    "\n",
    "# Efficient retreival of open buckets per device\n",
    "result = collection.create_index([(\"device\",pymongo.ASCENDING),\n",
    "                         (\"cnt\",pymongo.ASCENDING)],\n",
    "                        partialFilterExpression={\"cnt\": {\"$lt\":3}})\n",
    "print(\"Created Index: \" + result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These indexes will be used during the ingestion as well as the retreival process. And we will have a closer look at them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the Data\n",
    "\n",
    "With Aggregation Pipelines it is easy to query, filter, and format the data. This is the query for two timeseries (temperature and rpm). The sort should use the full index prefix in order to be executed on the index and not in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = collection.aggregate([\n",
    "  { \"$match\": { \"device\": 4711 } },\n",
    "  { \"$sort\": { \"device\": 1, \"min_ts\": 1 } },\n",
    "  { \"$unwind\": \"$m\" },\n",
    "  { \"$sort\": { \"m.ts\": 1 } },\n",
    "  { \"$project\": { \"_id\": 0, \"device\": 1, \"ts\": \"$m.ts\", \"temperature\": \"$m.temperature\", \"rpm\": \"$m.rpm\" } }\n",
    "]);\n",
    "   \n",
    "for doc in result:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to query for a certain timeframe, the following $match stage can be used to search for a certain timeframe (please replace LOWER_BOUND and UPPER_BOUND with appropriate ISODate values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOWER_BOUND = datetime.datetime(2020, 4, 20, 13, 26, 43, 18000) # Replace with lower bound (copy & paste from results above)\n",
    "UPPER_BOUND = datetime.datetime(2020, 4, 20, 13, 30, 26, 130000) # Replace with upper bound (copy & paste from results above)\n",
    "\n",
    "result = collection.aggregate([\n",
    "  { \"$match\": { \"device\": 4711, \"min_ts\": { \"$lte\": UPPER_BOUND }, \"max_ts\": { \"$gte\": LOWER_BOUND } } },\n",
    "  { \"$sort\": { \"device\": 1, \"min_ts\": 1 } },\n",
    "  { \"$unwind\": \"$m\" },\n",
    "  { \"$match\": { \"$and\": [ { \"m.ts\": { \"$lte\": UPPER_BOUND } }, { \"m.ts\": { \"$gte\": LOWER_BOUND } } ] } },\n",
    "  { \"$project\": { \"_id\": 0, \"device\": 1, \"ts\": \"$m.ts\", \"temperature\": \"$m.temperature\", \"rpm\": \"$m.rpm\" } }\n",
    "]);\n",
    "\n",
    "for doc in result:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to explain this query pattern\n",
    "\n",
    "We want to get the data from timestamps 8 to 17 that are spread across 5 buckets:\n",
    "```\n",
    "(1) 1 2 3 4 5\n",
    "(2) 6 7 8 9 10\n",
    "(3) 11 12 13 14 15\n",
    "(4) 16 17 18 19 20\n",
    "(5) 21 22 23 \n",
    "```\n",
    "\n",
    "We could use a complex condition, but this will end up in expensive index scans:\n",
    "```\n",
    "     min <= 8  and max >= 8   [ bucket (1) ]\n",
    " OR: min >= 8  and max <= 17  [ bucket (3) ]\n",
    " OR: min <= 17 and max >= 8   [ bucket (4) ]\n",
    "```\n",
    "\n",
    "The following statement leads to the same result and allows efficient index traversal and selects exactly the buckets of our interest:\n",
    "```\n",
    "     max >= 8\n",
    "AND: min <= 17\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
